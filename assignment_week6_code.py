# -*- coding: utf-8 -*-
"""Assignment_Week6_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f76cFhgNJCxmraxnvLzUvAxcGp6Vx5KO
"""

"""Patent Mining Project from Lens.org: Performing NLP to check what is the innovation landscape in "Quantum Computing" from top firms
like Google, IBM, Intel, Cisco, Amazon, Ionq, Bull SAS, Classiq, Fujitsu, etc. since 1 January 2025"""

#!pip install gensim
#!pip install gensim pyLDAvis nltk

#CODE BLOCK- IMPORTED ALL THE REQUIRED, NECESSARY LIBRARIES
try:
  import numpy as np
  import pandas as pd
  import csv
  import os
  import re
  import matplotlib.pyplot as plt
  import nltk
  from nltk.tokenize import word_tokenize
  from nltk.corpus import stopwords
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.decomposition import PCA
  from gensim import corpora, models
  from gensim.models import Word2Vec
  import pyLDAvis.gensim_models as gensimvis
  import pyLDAvis
  from IPython.display import display
  print("Libraries installed and imported successfully")

except ImportError:
  print("One or more libraries either not installed or not imported successfully.")

except ModuleNotFoundError:
  print("One or more libraries not found.")

except ValueError:
  print("Try installing and importing libraries in different cells")

else:
  print("All modules were imported and this is the 'else' code block executing.")

finally:
  print("Import completed.")

#Task Number 1: Text Preprocessing
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
custom_stopwords = {"using", "via", "based", "through", "approach", "method", "thereof", "systems", "methodology"} #Generic stopwords in patents unwanted in output
stop_words = set(stopwords.words('english')).union(custom_stopwords)

def text_cleaning(text):  #Function 'text_cleaning' performs text cleaning
  text = text.lower() #Converting each 'Title' to lowercase
  text = re.sub(r'[^a-zA-Z]',' ',text) #Removing punctuations, numbers with spaces
  words = text.split()  #Tokenizing
  words = [w for w in words if w not in stop_words] #Removing stopwords

  return words  #Returning tokens

#MAIN FUNCTION
def main():

  try:

    #PROCESSING THE TEXT FILE: 'Quantum-Computing.csv'
    df = pd.read_csv("Quantum-Computing.csv") #Passing the csv file as into a Pandas dataframe 'df'

    titles = df['Title'].dropna().astype(str).tolist()  #Passing the column 'Title' into a list 'titles'

    #Passing the list 'titles' as array 'tokenized_docs' into the function 'text_cleaning'
    tokenized_docs = [text_cleaning(t) for t in titles]

    #Task Number 2: TF-IDF Analysis
    #BAG-OF-WORDS REPRESENTATION & NUMERICAL VECTORIZATION

    #Creating dictionary to implment TF-IDF on the Bag-of-Words
    dictionary = corpora.Dictionary(tokenized_docs)

    #Creating corpus, that is, Bag-of-Words representation
    corpus_bow = [dictionary.doc2bow(w) for w in tokenized_docs]

    #Since TfidfVectorizer() expects a string and currently we have tuples, so it needs to be changed to string datatype
    corpus_str = [" ".join(doc) for doc in tokenized_docs]

    vectorizer = TfidfVectorizer()  #Creating object 'vectorizer' for the class 'TfidfVectorizer'

    obj_X = vectorizer.fit_transform(corpus_str) #Training the model

    feature_name = vectorizer.get_feature_names_out()

    tfidf_array = obj_X.toarray()

    #CHECKPOINTS
    print("\nFeature Names: ",feature_name) #Output the feature names
    print("\nMatrix View: ", tfidf_array)  #Output the TF-IDF scores for each word in the corpus

    for doc_idx, doc_vector in enumerate(tfidf_array): #enumerate(index,value)
      top_indices = doc_vector.argsort()[::-1][:10] #Arranging is descending order and provide the top 10
      top_terms = [(feature_name[i], doc_vector[i]) for i in top_indices] #Storing the sorted terms in top_terms
      print(f"\n Document {doc_idx + 1} 's top terms:")
      for term, score in top_terms:
        print(f"{term}: {score:.4f}") #Print term with tfidf score per document

    #Task Number 3: Word2Vec Embeddings
    #TRAINING WORD2VEC MODEL

    w2model = Word2Vec(
    sentences=tokenized_docs,  #Tokens passed
    vector_size=100, #Dimensions expected
    window=5,  #Maximum distance from the input word in the same context
    min_count=1, #Ignore rare words
    workers=4, #CPU of the system is quadcore
    sg=1  #Predicts from the Continuous Bag of Words in the same context to the target rather than the words from a similar context
    )

    #Checking Top 5 most similar words
    similarwords_1 = w2model.wv.most_similar('quantum',topn=5)  #For 'quantum'
    similarwords_2 = w2model.wv.most_similar('qubit',topn=5)  #For 'qubit'
    #The Word2Vec model is essential to suffix with .wv. It implements the query in read-only mode and without it we might get an error

    print(f"\nSimilar words for 'quantum': \n\n{similarwords_1}") #Output similar words for 'quantum'
    print(f"\nSimilar words for 'qubit': \n\n{similarwords_2}") #Output similar words for 'qubit'

    """Happy with the output because the model has identified the correct context for 'quantum' and 'qubit'"""

    #Input words for dimensionality reduction using PCA
    #WORDS TO VISUALIZE BASED ON THE PREVIOUS OUTPUT
    PCA_words = ["quantum", "qubits", "circuit", "error", "state", "computing",
    "optimization", "low"]

    #Getting embedding for the PCA_words
    embed_X = [w2model.wv[word] for word in PCA_words]

    """Applying PCA below. PCA stands for Principal Component Analysis. Each embedding is high-dimensional.
    In our case, it is 100. So, we use PCA to reduce the dimensionality for visualization, as we understand
    only 2-D and 3-D. Mathematically, PCA used eigenvector of covariance matrix to reduce the dimensionality."""

    #Reducing to 2-D for visualization
    pca = PCA(n_components=2)

    #Training the model on the word embeddings
    X_pca = pca.fit_transform(embed_X)

    #Plotting using Pyplot
    plt.figure(figsize=(8,6))
    plt.scatter(X_pca[:,0],X_pca[:,1],color="blue")  #Visualizing clusters
    for i, word in enumerate(PCA_words):
      plt.annotate(word,xy=(X_pca[i,0], X_pca[i,1]), fontsize=9, textcoords='offset points')

    plt.title("Word2Vec Embeddings (2D PCA Projections)")
    plt.show()

    #Task Number 4: Topic Modeling
    #Training the LDA Model with num_topics=3 over 15 iterations
    lda_model = models.LdaModel(corpus_bow, num_topics=3, id2word=dictionary, passes=15)

    #Printing topics
    for idx, topic in lda_model.print_topics(-1):
      print(f"Topic {idx+1}: {topic}")

    #Visualization
    pyLDAvis.enable_notebook()
    vis = gensimvis.prepare(lda_model, corpus_bow, dictionary)
    display(vis)

    """The dominant topic with maximum probability is Topic 1"""

    #Assigning topics to each document
    doc_topics = [] #empty list to store dominant topic for each doc
    for i, row in enumerate(lda_model[corpus_bow]): #Looping through each document
      #row is list of (topic_id, probability)
      row = sorted(row, key=lambda x: x[1], reverse=True)  #Sorting by probability in descending order for each document
      dominant_topic, topic_prob = row[0] #Selecting the most probable topic with probability for the particular document
      doc_topics.append([dominant_topic+1, topic_prob])  # +1 so topics are 1-3 not 0-2

    #Converting to DataFrame
    doc_topic_df = pd.DataFrame(doc_topics, columns=["Dominant_Topic", "Topic_Prob"])

    #Loading the original CSV
    df = pd.read_csv("Quantum-Computing.csv")

    #Appending topic info as new columns
    df["Dominant_Topic"] = doc_topic_df["Dominant_Topic"]
    df["Topic_Prob"] = doc_topic_df["Topic_Prob"]

    #Saving back to CSV
    df.to_csv("Quantum-Computing_with_topics.csv", index=False)
    #THEREFORE THE OUTPUT FILE IS "Quantum-Computing_with_topics.csv"

    #CHECKPOINT
    print(df.head(10))


  except FileNotFoundError:
    print("The input CSV file is missing")

#CALLING THE MAIN FUNCTION
main()

